import json
import yaml
import pandas as pd
from datetime import datetime, timezone

# 1) Load data
cmdb_assets = []
for path in glob.glob("cmdb/*.json"):
    with open(path) as f:
        cmdb_assets.extend(json.load(f)["assets"])

disc_df = pd.read_csv("discovery.csv", parse_dates=["last_seen"])

with open("rules.yml") as f:
    rules = yaml.safe_load(f)

# 2) Dedupe discovery by host+ip, prefer latest last_seen
disc_df = (disc_df
           .sort_values("last_seen")
           .drop_duplicates(subset=["host", "ip"], keep="last"))

# 3) Build candidate objects (rule-based)
candidates = []
now = datetime.now(timezone.utc)

for asset in cmdb_assets:
    host = asset["id"]
    ip = asset.get("ip")

    matches = disc_df[(disc_df["host"] == host) & (disc_df["ip"] == ip)]
    matches_dict = matches.to_dict(orient="records")

    candidates.append({
        "cmdb_record": asset,
        "discovery_records": matches_dict,
        "rules": rules
    })

# Also consider discovery-only hosts as NEW_ASSET candidates
cmdb_hosts = {a["id"] for a in cmdb_assets}
for _, row in disc_df.iterrows():
    if row["host"] not in cmdb_hosts:
        candidates.append({
            "cmdb_record": None,
            "discovery_records": [row.to_dict()],
            "rules": rules
        })

# 4) Call LLM per candidate
decisions = []
for c in candidates:
    prompt = build_prompt(c)  # format the JSON + instructions
    llm_output = call_llm(prompt)  # GenAI call (Bedrock/OpenAI/etc.)
    decisions.append(parse_llm_output(llm_output))

# 5) Render changeset markdown + evidence JSON
render_changeset_markdown(decisions, "cmdb_changeset.md")
with open("evidence_pack.json", "w") as f:
    json.dump(decisions, f, indent=2)
